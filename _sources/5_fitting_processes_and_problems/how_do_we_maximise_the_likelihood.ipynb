{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9468cf6-456c-48a0-ae9a-a89e95ac7628",
   "metadata": {},
   "source": [
    "# How do we maximise the likelihood?\n",
    "\n",
    "Now that there is a metric we will use to assess the quality of agreement between our model and our data, we can consider **how** we find the maximum likelihood for a given system.\n",
    "In this section, we will look at a couple of common methods for likelihood maximisation that are used in neutron reflectometry analysis. \n",
    "These methods take on two *flavours*, that we will investigate in turn. \n",
    "\n",
    "## Gradient methods\n",
    "\n",
    "The first method for likelihood maximisation that one will encounter in reflectometry analysis are **gradient** methods. \n",
    "These are optimisation algorithms that aim to maximise the likelihood by starting from the initial guess position and finding the local gradient. \n",
    "Once this is determined, the gradient method looks to move the current guess uphill to eventually reach some nearby maximum value.\n",
    "We can see a simple example of this below, where we find the maximum likelihood by a gradient method for a two-dimensional normal distribution. \n",
    "A range of gradient methods exist, but in general, they all try to find the maximum be considering local gradients.\n",
    "````{margin}\n",
    "```{note} \n",
    "Be aware, that many of these algorithms are implemented for **root-finding**, and therefore aim to minimise some function. Therefore, to maximise we simply minimize the negative of the function. \n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4c8c87-9c80-42d8-b1ed-6a47b15cdd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c0f5d1-e913-4850-9e70-309100d948b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mv = multivariate_normal([3.000, 1.000], [[0.033 , -0.167], [-0.167,  1.092]])\n",
    "gradient_v = []\n",
    "def gradient_callback(current_theta: np.ndarray) -> None:\n",
    "    gradient_v.append(current_theta)\n",
    "\n",
    "def nll(theta: np.ndarray) -> float:\n",
    "    return -mv.logpdf(theta)\n",
    "\n",
    "res = minimize(nll, [2.6, 3], callback=gradient_callback, method='Nelder-Mead')\n",
    "gradient_v = np.array(gradient_v)\n",
    "x = np.linspace(2.6, 3.4, 1000)\n",
    "y = np.linspace(-1.5, 3.5, 1000)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "plt.contourf(X, Y, mv.pdf(np.array([X, Y]).T).T)\n",
    "plt.plot(gradient_v[:, 0], gradient_v[:, 1], 'k-')\n",
    "plt.xlabel('$m$')\n",
    "plt.ylabel('$c$')\n",
    "plt.colorbar(label='$\\ln\\mathcal{L}$')\n",
    "plt.show()\n",
    "print('Maximum:', res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b122e1-455f-44a8-b43d-a0525fba1844",
   "metadata": {},
   "source": [
    "We can see above that the black line, indicating the path of the solution towards the maximum, moves in a rational path up towards the maximum, following the regions of an increasing gradient. \n",
    "Eventually, after a number of steps, the maximum is found.\n",
    "\n",
    "Gradient methods are effective for the discovery of local maxima, however, for more complex model-dependent analysis, this can be a problem. \n",
    "If the maximum that is found by the gradient method is not the global maxima, then it is not possible for these algorithms to easily go back \"downhill\". \n",
    "This means that the **starting position** of the algorithm is very important. \n",
    "\n",
    "Consider the two routes shown below on the multi-modal (more than one maximum) likelihood function. \n",
    "The blue line starts near to the local maximum at $(3, 0)$ and therefore the gradient approach is used, it is this maximum that is found. \n",
    "However, when the algorithm is started from $(10, 5)$ (orange line), the true global maximum at $(8, 2)$ is found instead. \n",
    "It is clear that for the **non-unique** analyses of neutron reflectometry, we desire a **global optimisation** approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716b8ae9-7289-4601-9bd6-21f560e829f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mv1 = multivariate_normal([3, 0], [[4, 2], [-2, 4]])\n",
    "mv2 = multivariate_normal([8, 2], [[2, 1], [1, 2]])\n",
    "\n",
    "def nll(theta: np.ndarray) -> float:\n",
    "    return -(mv1.pdf(theta) + mv2.pdf(theta))\n",
    "\n",
    "x = np.linspace(0, 11, 1000)\n",
    "y = np.linspace(-4, 5, 1000)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "plt.contourf(X, Y, mv1.pdf(np.array([X, Y]).T).T + mv2.pdf(np.array([X, Y]).T).T)\n",
    "gradient_v = []\n",
    "res1 = minimize(nll, [1, 3], callback=gradient_callback, method='Nelder-Mead')\n",
    "gradient_v = np.array(gradient_v)\n",
    "plt.plot(gradient_v[:, 0], gradient_v[:, 1], '-')\n",
    "gradient_v = []\n",
    "res2 = minimize(nll, [10, 5], callback=gradient_callback, method='Nelder-Mead')\n",
    "gradient_v = np.array(gradient_v)\n",
    "plt.plot(gradient_v[:, 0], gradient_v[:, 1], '-')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.colorbar(label='$\\ln\\mathcal{L}$')\n",
    "plt.show()\n",
    "print('Blue Maximum:', res1.x)\n",
    "print('Orange Maximum:', res2.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f13a5d6-bb90-493f-b4a3-12ac1c0712f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Stochastic methods\n",
    "\n",
    "The most common form of global optimisation methods are the stochastic methods, where stochastic means **random**.\n",
    "There are a variety of these methods, such as [particle swarm](https://en.wikipedia.org/wiki/Particle_swarm_optimization), which moves in the semi-random fashion analogous to a swarm of insects, or [evolutionary algorithms](https://en.wikipedia.org/wiki/Evolutionary_algorithm), which are inspired by biological evolution in order to find the optimum. \n",
    "The most commonly applied in neutron reflectometry model optimisation is the **differential evolution** method {cite}`storn_differential_1997,wormington_characterization_1999,bjork_fitting_2011`. \n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "The code used to implement a differential evolution algorithm can be found in the [`helper.py`](https://github.com/arm61/learn/blob/main/5_fitting_processes_and_problems/helper.py) module.\n",
    "```\n",
    "````\n",
    "In the figure below, we show the application of a differential evolution algorithm with a population of $8$ (however, the optimisation route of the $4$ best members are shown). \n",
    "It is clear that the methodology results in a more complete interrogation of the complex optimisation space. \n",
    "Typically once a solution is found using the differential evolution is found, this is **further optimised** with a gradient descent method to find the maximum global likelihood. \n",
    "This is necessary as, as we can see from the overall maximum, the differential evolution will not find the absolute maximum alone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d8164a-410d-4047-9854-1a165abf6913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import differential_evolution\n",
    "\n",
    "x = np.linspace(0, 11, 1000)\n",
    "y = np.linspace(-4, 5, 1000)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "plt.contourf(X, Y, mv1.pdf(np.array([X, Y]).T).T + mv2.pdf(np.array([X, Y]).T).T)\n",
    "res = differential_evolution(nll, ((0, 11), (-4, 5)))\n",
    "top4 = np.argsort(nll(res[-1].T))[:4]\n",
    "plt.plot(res[:, 0, top4], res[:, 1, top4], '-')\n",
    "plt.xlabel('$m$')\n",
    "plt.ylabel('$c$')\n",
    "plt.colorbar(label='$\\ln\\mathcal{L}$')\n",
    "plt.show()\n",
    "print('Overall Maximum:', res[-1].T[np.argmax(nll(res[-1].T))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df49afd-fc72-4d66-9117-0ff1f8167392",
   "metadata": {},
   "source": [
    "It is important to note that as the dimensionality to a fitting problem increases, as the number of free parameters in our model increases, it will become harder and harder to find an optimised solution. \n",
    "Therefore, we should carefully consider which parameters in a model are allowed to vary in the fitting process, ensuring that the minimum number of parameters are changing. \n",
    "There are some important considerations that feed into this, for example, that having **more isotopic contrasts** of neutron reflectometry data means that more parameters can be varied. \n",
    "\n",
    "Once some maximum likelihood is found, it is possible to probe the shape of that maximum and draw conclusions about the robustivity of the result. \n",
    "Additionally, the maximum number of free parameters that a given dataset(s) can describe is quantifiable, however, very computationally intensive {cite}`treece_optimization_2019`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
